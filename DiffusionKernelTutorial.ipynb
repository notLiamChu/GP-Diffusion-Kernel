{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difffusion Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the training data and some convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test to ensure kernel is working\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Training data \n",
    "train_x = torch.Tensor(np.array([0, 1, 2]))\n",
    "# True function is 2 * x with Gaussian noise\n",
    "train_y = 2*train_x + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor([[1, 2], [2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap training, prediction and plotting into a function,\n",
    "# so that we do not have to repeat the code later on\n",
    "def train(model, likelihood, training_iter=training_iter):\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "def predict(model, likelihood, test_x):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        return likelihood(model(test_x))\n",
    "    \n",
    "def plot(observed_pred, test_x):\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "        # Get upper and lower confidence bounds\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        # Plot training data as black stars\n",
    "        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "        # Plot predictive means as blue line\n",
    "        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "        # Shade between the lower and upper confidence bounds\n",
    "        ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "        ax.set_ylim([-3, 3])\n",
    "        ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model using Aryan's (WSU Ph.D student) code for Diffusion Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionKernel(gpytorch.kernels.Kernel):\n",
    "    r\"\"\"\n",
    "        Computes diffusion kernel over discrete spaces with arbitrary number of categories. \n",
    "        Input type: n dimensional discrete input with c_i possible categories/choices for each dimension i \n",
    "        As an example, binary {0,1} combinatorial space corresponds to c_i = 2 for each dimension i\n",
    "        References:\n",
    "        - https://www.ml.cmu.edu/research/dap-papers/kondor-diffusion-kernels.pdf (Section 4.4)\n",
    "        - https://arxiv.org/abs/1902.00448\n",
    "        - https://arxiv.org/abs/2012.07762\n",
    "        \n",
    "        Args:\n",
    "        :attr:`categories`(tensor, list):\n",
    "            array with number of possible categories in each dimension            \n",
    "    \"\"\"\n",
    "    has_lengthscale = True\n",
    "    def __init__(self, categories, **kwargs):\n",
    "        if categories is None:\n",
    "            raise RunTimeError(\"Can't create a diffusion kernel without number of categories. Please define them!\")\n",
    "        super().__init__(**kwargs)\n",
    "        self.cats = categories\n",
    "\n",
    "    def forward(self, x1, x2, diag: Optional[bool] = False, last_dim_is_batch: Optional[bool] = False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n",
    "            x2 = x2.transpose(-1, -2).unsqueeze(-1)        \n",
    "\n",
    "        if diag:\n",
    "            res = 1.\n",
    "            for i in range(x1.shape[~1]):\n",
    "                res *= ((1 - torch.exp(-self.lengthscale[..., i] * self.cats[i]))/(1 + (self.cats[i] - 1) * torch.exp(-self.lengthscale[..., i]*self.cats[i]))).unsqueeze(-1) ** ((x1[..., i] != x2[..., i])[:, 0, ...])\n",
    "            return res\n",
    "\n",
    "        res = 1.\n",
    "        for i in range(x1.shape[~1]): \n",
    "            res *= ((1 - torch.exp(-self.lengthscale[..., i] * self.cats[i]))/(1 + (self.cats[i] - 1) * torch.exp(-self.lengthscale[..., i]*self.cats[i]))).unsqueeze(-1) ** ((x1[..., i].unsqueeze(-2)[..., None] != x2[..., i].unsqueeze(-2))[0, ...])\n",
    "        return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate our own custom model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = DiffusionKernel(categories=[[2], [2]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train and evaluate the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[497], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m likelihood\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m train(model, likelihood, test_data)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Get into evaluation (predictive posterior) mode and predict\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[494], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, likelihood, training_iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# \"Loss\" for GPs - the marginal log likelihood\u001b[39;00m\n\u001b[1;32m      8\u001b[0m mll \u001b[39m=\u001b[39m gpytorch\u001b[39m.\u001b[39mmlls\u001b[39m.\u001b[39mExactMarginalLogLikelihood(likelihood, model)\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(training_iter):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Zero gradients from previous iteration\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Output from model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = DiffGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Testing Data\n",
    "test_data = torch.Tensor([[2, 4], [6, 8]])\n",
    "\n",
    "# Set to training mode and train\n",
    "model.train()\n",
    "likelihood.train()\n",
    "train(model, likelihood, test_data)\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode and predict\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "observed_pred = predict(model, likelihood, test_data)\n",
    "\n",
    "# Plot results\n",
    "plot(observed_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
