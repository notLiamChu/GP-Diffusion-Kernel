{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difffusion Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up the training data and some convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Smoke test to ensure kernel is working\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Training data is simple square matrix of 1's\n",
    "train_x = torch.tensor(np.array([[1, 0], [0, 1]]), dtype=torch.float32)\n",
    "print(train_x)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(2 * math.pi * train_x) + torch.randn(train_x.size()) * 0.2\n",
    "\n",
    "# Wrap training, prediction and plotting from the ExactGP-Tutorial into a function,\n",
    "# so that we do not have to repeat the code later on\n",
    "def train(model, likelihood, training_iter=training_iter):\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "def predict(model, likelihood, test_x = torch.linspace(0, 1, 51)):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        return likelihood(model(test_x))\n",
    "    \n",
    "def plot(observed_pred, test_x=torch.linspace(0, 1, 51)):\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "        # Get upper and lower confidence bounds\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        # Plot training data as black stars\n",
    "        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "        # Plot predictive means as blue line\n",
    "        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "        # Shade between the lower and upper confidence bounds\n",
    "        ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "        ax.set_ylim([-3, 3])\n",
    "        ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model using Aryan's (WSU Ph.D student) code for Diffusion Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionKernel(gpytorch.kernels.Kernel):\n",
    "    r\"\"\"\n",
    "        Computes diffusion kernel over discrete spaces with arbitrary number of categories. \n",
    "        Input type: n dimensional discrete input with c_i possible categories/choices for each dimension i \n",
    "        As an example, binary {0,1} combinatorial space corresponds to c_i = 2 for each dimension i\n",
    "        References:\n",
    "        - https://www.ml.cmu.edu/research/dap-papers/kondor-diffusion-kernels.pdf (Section 4.4)\n",
    "        - https://arxiv.org/abs/1902.00448\n",
    "        - https://arxiv.org/abs/2012.07762\n",
    "        \n",
    "        Args:\n",
    "        :attr:`categories`(tensor, list):\n",
    "            array with number of possible categories in each dimension            \n",
    "    \"\"\"\n",
    "    has_lengthscale = True\n",
    "    def __init__(self, categories, **kwargs):\n",
    "        if categories is None:\n",
    "            raise RunTimeError(\"Can't create a diffusion kernel without number of categories. Please define them!\")\n",
    "        super().__init__(**kwargs)\n",
    "        self.cats = categories\n",
    "\n",
    "    def forward(self, x1, x2, diag: Optional[bool] = False, last_dim_is_batch: Optional[bool] = False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n",
    "            x2 = x2.transpose(-1, -2).unsqueeze(-1)        \n",
    "\n",
    "        if diag:\n",
    "            res = 1.\n",
    "            for i in range(x1.shape[-1]):\n",
    "                res *= ((1 - torch.exp(-self.lengthscale[..., i] * self.cats[i]))/(1 + (self.cats[i] - 1) * torch.exp(-self.lengthscale[..., i]*self.cats[i]))).unsqueeze(-1) ** ((x1[..., i] != x2[..., i])[:, 0, ...])\n",
    "            return res\n",
    "\n",
    "        res = 1.\n",
    "        for i in range(x1.shape[-1]): \n",
    "            res *= ((1 - torch.exp(-self.lengthscale[..., i] * self.cats[i]))/(1 + (self.cats[i] - 1) * torch.exp(-self.lengthscale[..., i]*self.cats[i]))).unsqueeze(-1) ** ((x1[..., i].unsqueeze(-2)[..., None] != x2[..., i].unsqueeze(-2))[0, ...])\n",
    "        return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate our own custom model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = DiffusionKernel(categories=torch.tensor(np.array([[2], [2]])))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train and evaluate the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m likelihood\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m train(model, likelihood)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Get into evaluation (predictive posterior) mode and predict\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, likelihood, training_iter)\u001b[0m\n\u001b[1;32m     25\u001b[0m output \u001b[39m=\u001b[39m model(train_x)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output, train_y)\n\u001b[1;32m     28\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood(function_dist, \u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mlog_prob(target)\n\u001b[1;32m     65\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/distributions/multivariate_normal.py:192\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    185\u001b[0m         covar \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39mrepeat(\n\u001b[1;32m    186\u001b[0m             \u001b[39m*\u001b[39m(diff_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m covar_size \u001b[39mfor\u001b[39;00m diff_size, covar_size \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(diff\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], padded_batch_shape)),\n\u001b[1;32m    187\u001b[0m             \u001b[39m1\u001b[39m,\n\u001b[1;32m    188\u001b[0m             \u001b[39m1\u001b[39m,\n\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    191\u001b[0m \u001b[39m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m covar \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39;49mevaluate_kernel()\n\u001b[1;32m    193\u001b[0m inv_quad, logdet \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[39m=\u001b[39mdiff\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), logdet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([inv_quad, logdet, diff\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpi)])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1470\u001b[0m, in \u001b[0;36mLinearOperator.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_kernel\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1466\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[39m    Return a new LinearOperator representing the same one as this one, but with\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[39m    all lazily evaluated kernels actually evaluated.\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_tree()(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1965\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepresentation_tree\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[1;32m   1956\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[39m    Returns a\u001b[39;00m\n\u001b[1;32m   1958\u001b[0m \u001b[39m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[39m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1965\u001b[0m     \u001b[39mreturn\u001b[39;00m LinearOperatorRepresentationTree(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/linear_operator/operators/linear_operator_representation_tree.py:13\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m linear_op\u001b[39m.\u001b[39m_args:\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(arg, \u001b[39m\"\u001b[39m\u001b[39mrepresentation\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m callable(arg\u001b[39m.\u001b[39mrepresentation):  \u001b[39m# Is it a lazy tensor?\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m         representation_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(arg\u001b[39m.\u001b[39;49mrepresentation())\n\u001b[1;32m     14\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     15\u001b[0m             (\n\u001b[1;32m     16\u001b[0m                 \u001b[39mslice\u001b[39m(counter, counter \u001b[39m+\u001b[39m representation_size, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m                 arg\u001b[39m.\u001b[39mrepresentation_tree(),\n\u001b[1;32m     18\u001b[0m             )\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m representation_size\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/linear_operator/operators/_linear_operator.py:1949\u001b[0m, in \u001b[0;36mLinearOperator.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     representation\u001b[39m.\u001b[39mappend(arg)\n\u001b[1;32m   1948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(arg, \u001b[39m\"\u001b[39m\u001b[39mrepresentation\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m callable(arg\u001b[39m.\u001b[39mrepresentation):  \u001b[39m# Is it a LinearOperator?\u001b[39;00m\n\u001b[0;32m-> 1949\u001b[0m     representation \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(arg\u001b[39m.\u001b[39;49mrepresentation())\n\u001b[1;32m   1950\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1951\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRepresentation of a LinearOperator should consist only of Tensors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrepresentation()\n\u001b[1;32m    394\u001b[0m \u001b[39m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# representation\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_kernel()\u001b[39m.\u001b[39mrepresentation()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache(\u001b[39mself\u001b[39m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     temp_active_dims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel(\n\u001b[1;32m    356\u001b[0m         x1,\n\u001b[1;32m    357\u001b[0m         x2,\n\u001b[1;32m    358\u001b[0m         diag\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    359\u001b[0m         last_dim_is_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_dim_is_batch,\n\u001b[1;32m    360\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams,\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims \u001b[39m=\u001b[39m temp_active_dims\n\u001b[1;32m    364\u001b[0m \u001b[39m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:524\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    521\u001b[0m     res \u001b[39m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, last_dim_is_batch\u001b[39m=\u001b[39mlast_dim_is_batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m    522\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     res \u001b[39m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 524\u001b[0m         \u001b[39msuper\u001b[39;49m(Kernel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x1_, x2_, last_dim_is_batch\u001b[39m=\u001b[39;49mlast_dim_is_batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mDiffusionKernel.forward\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m     33\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x1\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]): \n\u001b[0;32m---> 35\u001b[0m     res \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlengthscale[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, i] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcats[i]))\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcats[i] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengthscale[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i]\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcats[i])))\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m ((x1[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m] \u001b[39m!=\u001b[39m x2[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))[\u001b[39m0\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = DiffGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "# Set to training mode and train\n",
    "model.train()\n",
    "likelihood.train()\n",
    "train(model, likelihood)\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode and predict\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "observed_pred = predict(model, likelihood)\n",
    "\n",
    "# Plot results\n",
    "plot(observed_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
